---
permalink: /
title: "Hello!"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm doing a MSc (Research) in Computer Science at <a href="https://mila.quebec/en">MILA</a> and McGill University, with <a href="https://www.cs.mcgill.ca/~jcheung/">Professor Jackie Cheung</a>. I work on natural language processing, and how to train models with scarce data.

**Past Research**: Previously, I worked on NLP for summarization and simplification (with <a href="https://armancohan.com/">Prof. Arman Cohan</a>), and tabular data (with <a href="https://www.cs.yale.edu/homes/radev/">Prof. Dragomir Radev</a>, <a href="https://linyongnan.github.io/">Linyong Nan</a>) at Yale.

**Work Experience**: I've interned at Adobe (2025, AI Applied Research) and Elicit (2024, ML Engg). Before that, I was a data scientist at McKinsey and Company, QuantumBlack (2023-2024).

WriteDoc
=====
I'm working on **WriteDoc** (check us out at write-doc.com!), a medical scribe + paperwork tool for Filipino/Taglish! We're piloting with various doctors + clinics – would love to chat if you're interested in healthcare x AI.

Selected Publications
======
* <b>Lorenzo Flores</b>, Junyi Shen, Goodman Gu. Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning, ArXiv [<a href="https://arxiv.org/abs/2508.11120">Paper</a>]
* <b>Lorenzo Flores</b>, Ori Ernst, Jackie Chi Kit Cheung. Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution's Characteristics, ACL 2025 [<a href="https://arxiv.org/abs/2506.00637">Paper</a>, <a href="https://github.com/ljyflores/calibrated-confidence-for-nlg">Code</a>]
* <b>Lorenzo Flores</b> and Arman Cohan. On the Benefits of Fine-Grained Loss Truncation: A Case Study on Factuality in Summarization, EACL 2024 [<a href="https://aclanthology.org/2024.eacl-short.13/">Paper</a>, <a href="https://drive.google.com/file/d/17K8AksYCXYQd7vKAjdbHMmKRyn3daaPZ/view?usp=sharing">Video</a>, <a href="https://github.com/yale-nlp/Simplification-Projects">Code</a>]
* <b>Lorenzo Flores</b>, Heyuan Huang, Kejian Shi, Sophie Chheang, and Arman Cohan. 2023. Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding, EMNLP 2023 Findings [<a href="https://aclanthology.org/2023.findings-emnlp.322/">Paper</a>, <a href="https://drive.google.com/file/d/1NBWzFTFdtmR2gL2Sq_ah-Dwuz8rME6Zc/view?usp=sharing">Video</a>, <a href="https://github.com/yale-nlp/Simplification-Projects">Code</a>, <a href="https://huggingface.co/spaces/ljyflores/simplification-model-app">Demo</a>]
* Linyong Nan, <b>Lorenzo Flores</b>, Yilun Zhao, Yixin Liu, Luke Benson, Weijin Zou, and Dragomir Radev. 2022. R2D2: Robust Data-to-Text with Replacement Detection, EMNLP 2022 [<a href="https://aclanthology.org/2022.emnlp-main.464/">Paper</a>]
* <b>Lorenzo Flores</b>, Dragomir Radev. 2022. Look Ma, Only 400 Samples! Revisiting the Effectiveness of Automatic N-Gram Rule Generation for Spelling Normalization in Filipino, EMNLP 2022 SustaiNLP Workshop [<a href="https://aclanthology.org/2022.sustainlp-1.5/">Paper</a>, <a href="https://aclanthology.org/2022.sustainlp-1.5.mp4">Video</a>, <a href="https://github.com/ljyflores/efficient-spelling-normalization-filipino">Code</a>]
* <b>Lorenzo Flores</b>, Yiding Hao. 2022. Adversarial Benchmark for Fake News Classification. AAAI 2022 AdvML Workshop [<a href="https://arxiv.org/abs/2201.00912">Paper</a>, <a href="https://github.com/ljyflores/fake-news-adversarial-benchmark">Code</a>]
* Chiara Ledesma, Oshean Lee Garonita, <b>Lorenzo Flores</b>, Isabelle Tingzon, and Danielle Dalisay. 2020. Interpretable Poverty Mapping using Social Media Data, Satellite Images, and Geospatial Information, ML4D Workshop, NeurIPS 2020, Best Workshop Paper Award [<a href="https://arxiv.org/abs/2011.13563">Paper</a>]

Projects
======
* `LossLibrary`: a repository that consolidates loss functions from NLP literature and helps users integrate it into training [<a href="https://github.com/ljyflores/loss-library">Here!</a>]
